{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81abe8f0-da4b-4b33-9194-ffa639f33dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import expr,to_date,col\n",
    "\n",
    "\n",
    "\n",
    "df_src1_raw = spark.read.options(header=True,inferSchema=True).format(\"csv\").load(\"/Volumes/workspace/logistics_data/logistics_volume/logistics_source1\")\n",
    "\n",
    "df_src2_raw = spark.read.options(header=True,inferSchema=True).format(\"csv\").load(\"/Volumes/workspace/logistics_data/logistics_volume/logistics_source2\")\n",
    "\n",
    "df_logistics_ship_raw = spark.read.options(header=True,inferSchema=True,multiline=True).format(\"json\").load(\"/Volumes/workspace/logistics_data/logistics_volume/logistics_shipment_detail_3000.json\")\n",
    "\n",
    "\n",
    "print('count of df_src1_raw:-',df_src1_raw.count())\n",
    "print('count of df_src2_raw:-',df_src2_raw.count())\n",
    "print('count of df_json_raw:-',df_logistics_ship_raw.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff32b8e-aedc-40f9-a362-6cc52a40df9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "# Task 1 and 2\n",
    "\n",
    "from pyspark.sql.functions import lit,to_date,col,expr,when,lower,upper,cast,current_timestamp, initcap,count\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "df_src1_clned = df_src1_raw.withColumn(\"source\",lit(\"system1\"))\n",
    "df_src2_clned = df_src2_raw.withColumn(\"source\",lit(\"system2\"))\n",
    "\n",
    "df_cust_combined = df_src1_clned.unionByName(df_src2_clned,allowMissingColumns = True).select(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\",\"hub_location\",\"vehicle_type\",\"source\")\n",
    "\n",
    "df_customer = df_cust_combined.dropna(subset=[\"shipment_id\",\"role\"]) \\\n",
    "    .filter(col(\"first_name\").isNotNull() | col(\"last_name\").isNotNull()) \\\n",
    "    .filter(col(\"shipment_id\").rlike(\"^[0-9]+$\")) \\\n",
    "    .dropDuplicates([\"shipment_id\"]) \\\n",
    "    .withColumn(\"age\",when(col(\"age\").rlike(\"^[0-9]+$\"), col(\"age\").cast(\"int\")).otherwise(-1)) \\\n",
    "    .withColumn(\"vehicle_type\",when (col(\"vehicle_type\").isNull(),\"UNKNOWN\")\n",
    "    .when (lower(col(\"vehicle_type\")) == \"truck\", \"LMV\")\n",
    "    .when (lower(col(\"vehicle_type\")) == \"bike\", \"TwoWheeler\") \\\n",
    "    .otherwise(col(\"vehicle_type\"))) \\\n",
    "    .withColumn(\"role\",lower(col(\"role\"))) \\\n",
    "    .withColumn(\"vehicle_type\",upper(col(\"role\"))) \\\n",
    "    .withColumn(\"hub_location\",initcap(col(\"hub_location\"))) \\\n",
    "    .withColumnsRenamed({\"hub_location\":\"origin_hub_city\",\"first_name\":\"staff_first_name\",\"last_name\":\"staff_last_name\"})\n",
    "\n",
    "df_customer=df_customer.dropDuplicates()\n",
    "df_customer=df_customer.dropDuplicates(subset=[\"shipment_id\"])\n",
    "\n",
    "# df_customer.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5cdd3f-189d-4cd1-9584-166a4b4c7f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "df_log_data_stand =df_logistics_ship_raw.withColumn(\"domain\", lit(\"logistics\")) \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"is_expedited\", lit(\"False\")) \\\n",
    "    .withColumn(\"shipment_date\", to_date(col(\"shipment_date\"), 'yyyy-MM-dd')) \\\n",
    "    .withColumn(\"shipment_cost\", col(\"shipment_cost\").cast(DecimalType(15, 2))) \\\n",
    "    .withColumn(\"shipment_weight_kg\", col(\"shipment_weight_kg\").cast(\"double\"))\n",
    "\n",
    "\n",
    "df_trans_data=df_log_data_stand.withColumn(\"is_expedited\",col(\"is_expedited\") \\\n",
    "    .cast(\"boolean\"))\n",
    "# de duplication\n",
    "df_trans_data=df_trans_data.dropDuplicates()\n",
    "\n",
    "\n",
    "# df_trans_data.display()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fb79d1-bff6-47aa-9ead-f08cba831dda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Data Enrichment - Detailing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b18b7df-d753-42e3-8e97-6298f70f3365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws,year,month,dayofweek,day\n",
    "\n",
    "df_cust_enrich = df_customer.withColumn(\"load_dt\",current_timestamp()) \\\n",
    "    .withColumn(\"full_name\",concat_ws(\" \",col(\"staff_first_name\"),col(\"staff_last_name\")))\n",
    "\n",
    "\n",
    "df_trans_enrich = df_trans_data \\\n",
    "    .withColumn(\"route_segment\",concat_ws(\"-\",col(\"source_city\"),col(\"destination_city\"))) \\\n",
    "    .withColumn(\"vehicle_identifier\",concat_ws(\"_\",col(\"vehicle_type\"),col(\"shipment_id\"))) \\\n",
    "    .withColumn(\"shipment_year\",year(col(\"shipment_date\"))) \\\n",
    "    .withColumn(\"shipment_month\",month(col(\"shipment_date\"))) \\\n",
    "    .withColumn(\"shipment_day\",day(col(\"shipment_date\"))) \\\n",
    "    .withColumn(\"is_weekend\",dayofweek(col(\"shipment_date\")).isin(1,7)) \\\n",
    "    .withColumn(\"is_expedited\",when(col(\"shipment_status\").isin (\"IN_TRANSIT\",\"DELIVERED\"),lit(\"TRUE\")).otherwise(\"FALSE\"))\n",
    "    \n",
    "# df_trans_enrich=df_trans_enrich.dropDuplicates)\n",
    "\n",
    "# df_cust_enrich.display()\n",
    "# df_trans_enrich.display()\n",
    "# df_trans_enrich.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64b0ca51-dedf-4ab7-b939-0944d299a9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Enrichment/Business Logics (Calculated Fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "accf275e-a2a3-43e7-8d31-8d0130b67858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff,current_date\n",
    "\n",
    "df_trans_enrich_bl = df_trans_enrich.withColumn(\"cost_per_kg\",col(\"shipment_cost\")/col(\"shipment_weight_kg\")) \\\n",
    "    .withColumn(\"days_since_shipment\",datediff(current_date(),col(\"shipment_date\"))) \\\n",
    "    .withColumn(\"tax_amount\",col(\"shipment_cost\")*lit(0.18))\n",
    "\n",
    "# df_trans_enrich_bl.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72bed6ec-8997-4b81-ad41-3c7dc557e663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Remove/Eliminate (drop, select, selectExpr)<br>\n",
    " UDF1: Complex Incentive Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d89808c-2ef5-4814-9e8b-d9dfa30b9c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, substring, concat, repeat, length\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "def calculate_bonus(role, age):\n",
    "    if role is not None and isinstance(role, str):\n",
    "        if role.lower() == \"driver\" and age is not None:\n",
    "            if age >= 50:\n",
    "                return 15\n",
    "            elif age < 30:\n",
    "                return 5\n",
    "    return 0\n",
    "\n",
    "def mask_identity(val):\n",
    "    if val is not None and len(val) > 2:\n",
    "        return val[:2] + \"*\" * (len(val) - 2)\n",
    "    elif val is not None:\n",
    "        return val\n",
    "    return None\n",
    "\n",
    "bonus_udf = udf(calculate_bonus, IntegerType())\n",
    "mask_identity_udf = udf(mask_identity, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa3601e-84be-40eb-bb05-b0765b67da7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split,substring,length,day\n",
    "\n",
    "df_cust_en_clean = df_cust_enrich.drop(\"staff_first_name\",\"staff_last_name\") \\\n",
    "    .withColumn(\"projected_bonus\",bonus_udf(col(\"role\"),col(\"age\"))) \\\n",
    "    .withColumn(\"masked_name\",mask_identity_udf(\"full_name\"))\n",
    "\n",
    "df_trans_enrich_bl1 = df_trans_enrich_bl.withColumn(\"order_prefix\",substring(col(\"order_id\"),1,3)) \\\n",
    "    .withColumn(\"order_sequence\",substring(col(\"order_id\"),4,100)) \\\n",
    "    .withColumn(\"route_lane\",concat_ws(\"->\",col(\"source_city\"),col(\"destination_city\")))\n",
    "# df_cust_en_clean.display()\n",
    "# df_trans_enrich_bl1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5805ead-71a3-48a0-b0c7-75476517480c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Data Core Curation & Processing (Pre-Wrangling)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "logistics_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

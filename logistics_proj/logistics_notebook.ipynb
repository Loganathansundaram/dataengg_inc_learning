{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23086fe6-a91f-4d62-85c6-1e3e59016dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import expr,to_date,col\n",
    "\n",
    "# read the files put into dataframe \n",
    "# also if the age is null the replace with 77\n",
    "\n",
    "df_src1_raw = spark.read.options(header=True,inferSchema=True).format(\"csv\").load(\"/Volumes/workspace/logistics_data/logistics_volume/logistics_source1\").withColumn(\"age\",expr(\"case when try_cast(age as INT) is null then -1 else age end\")).filter(expr(\"try_cast(shipment_id as double) IS not NULL and try_cast(age as INT) IS not NULL\")).distinct()\n",
    "\n",
    "df_src2_raw = spark.read.options(header=True,inferSchema=True).format(\"csv\").load(\"/Volumes/workspace/logistics_data/logistics_volume/logistics_source2\").withColumn(\"age\",expr(\"case when try_cast(age as INT) is null then -1 else age end\")).filter(expr(\"try_cast(shipment_id as double) IS not NULL and try_cast(age as INT) IS not NULL\")).distinct()\n",
    "\n",
    "df_json_raw = spark.read.options(header=True,inferSchema=True,multiline=True).format(\"json\").load(\"/Volumes/workspace/logistics_data/logistics_volume/logistics_shipment_detail_3000.json\").withColumn(\"shipment_date\",to_date(col(\"shipment_date\"),\"yyyy-mm-dd\")).distinct()\n",
    "\n",
    "\n",
    "print('count of df_src1_raw:-',df_src1_raw.count())\n",
    "print('count of df_src2_raw:-',df_src2_raw.count())\n",
    "print('count of df_json_raw:-',df_json_raw.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff32b8e-aedc-40f9-a362-6cc52a40df9d",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768123243290}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768222083622}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,to_date\n",
    "\n",
    "df_src1_clned = df_src1_raw.withColumn(\"source\",lit(\"source1\"))\n",
    "df_src2_clned = df_src2_raw.withColumn(\"source\",lit(\"source2\"))\n",
    "# df_json_clned = df_json_raw.withColumn(\"source\",lit(\"source3\"))\n",
    "\n",
    "df_src_combined = df_src1_clned.unionByName(df_src2_clned,allowMissingColumns = True).select(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\",\"hub_location\",\"vehicle_type\",\"source\").distinct()\n",
    "\n",
    "# df_src_combined.select(\"shipment_id\").groupBy(\"shipment_id\").count().display()#.filter(col(\"count\")>1).display()\n",
    "\n",
    "df_src_combined.dropna(subset=[\"shipment_id\",\"role\"]).filter(col(\"first_name\").isNotNull() & col(\"last_name\").isNotNull()).display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5647a1d7-62c7-4bf7-9ee0-7aca83b045ae",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768052821895}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType,FloatType\n",
    "\n",
    "src1_schema = StructType([\n",
    "    StructField(\"shipment_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"role\", StringType(), True)\n",
    "])\n",
    "\n",
    "src2_schema = StructType([\n",
    "    StructField(\"shipment_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"role\", StringType(), True),\n",
    "    StructField(\"hub_location\", StringType(), True),\n",
    "    StructField(\"vehicle_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "srcjson_schema = StructType([\n",
    "    StructField(\"cargo_type\", StringType(), True),\n",
    "    StructField(\"destination_city\", StringType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"payment_mode\", StringType(), True),\n",
    "    StructField(\"shipment_cost\", FloatType(), True),\n",
    "    StructField(\"shipment_date\", StringType(), True),\n",
    "    StructField(\"shipment_id\", IntegerType(), True),\n",
    "    StructField(\"shipment_status\", StringType(), True),\n",
    "    StructField(\"shipment_weight_kg\", FloatType(), True),\n",
    "    StructField(\"source_city\", StringType(), True),\n",
    "    StructField(\"vehicle_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df_src1 = spark.read.schema(src1_schema).options(header=True).format(\"csv\").load(\"/Volumes/workspace/logistics_data/logistics_volume/logistics_source1\")\n",
    "df_src2 = spark.read.schema(src2_schema).options(header=True,inferSchema=True).format(\"csv\").load(\"/Volumes/workspace/logistics_data/logistics_volume/logistics_source2\")\n",
    "df_json = spark.read.schema(srcjson_schema).option(\"multiline\", True).format(\"json\").load(\"/Volumes/workspace/logistics_data/logistics_volume/logistics_shipment_detail_3000.json\")\n",
    "\n",
    "\n",
    "print(\"df_src1 count\",df_src1.count())\n",
    "print(\"df_src1 distinct count\",df_src1.distinct().count())\n",
    "print(\"df_src2 count\",df_src2.count())\n",
    "print(\"df_src2 distinct count\",df_src1.distinct().count())\n",
    "print(\"df_json count\",df_json.count())\n",
    "print(\"df_json distinct count\",df_src1.distinct().count())\n",
    "\n",
    "# df_src1.printSchema()\n",
    "# df_src2.printSchema()\n",
    "# df_json.printSchema()\n",
    "\n",
    "# display(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68fbb68c-0e89-46e0-9638-f7afe12bb85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_src1.select(\"*\").filter(df_src1.shipment_id.isNull()))\n",
    "display(df_src2.select(\"*\").filter(df_src2.shipment_id.isNull()))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "logistics_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

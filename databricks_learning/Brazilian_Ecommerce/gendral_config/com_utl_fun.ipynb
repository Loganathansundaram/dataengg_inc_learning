{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a043d69a-6e35-4d80-9b5a-d7cc957679ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Return Spark session\n",
    "from pyspark.sql.session import SparkSession\n",
    "def get_spark_session(app_name=\"Some Anonymous Data Engineering Project\"):\n",
    "    try:\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark:\n",
    "            return spark\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (SparkSession.builder.appName(app_name).getOrCreate())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ed31e4-82c7-4cfb-b0d4-0ca19cd4f460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_file(spark,filetype,path,header=True,infer_schema=True,mline=True):\n",
    "    if filetype==\"csv\":\n",
    "        return spark.read.csv(path,header=header,inferSchema=infer_schema)#read_csv_df(spark,path)\n",
    "    elif filetype==\"json\":\n",
    "        return read_json_df(spark,path)\n",
    "    elif filetype==\"delta\":\n",
    "        return read_delta_df(spark,path)\n",
    "    elif filetype=='orc':\n",
    "        return spark.read.orc(path)\n",
    "    elif filetype=='parquet':\n",
    "        return spark.read.parquet(path)\n",
    "    else:\n",
    "        raise Exception(\"File type not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9111c9a1-2d8b-468a-8724-cf4cf34099ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_file(df, path,format=\"delta\", mode=\"overwrite\"):\n",
    "    return df.write.mode(mode).format(format).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21501e22-e056-4e05-8e96-2dec1decbed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def enrich_df(df):\n",
    "    return(df.withColumn(\"ingestion_time\",current_timestamp()))\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def enrich_add_col(df, columns):\n",
    "    for cname, props in columns.items():\n",
    "        dtype = props.get(\"type\")\n",
    "        default = props.get(\"default\")\n",
    "\n",
    "        if dtype == \"string\":\n",
    "            df = df.withColumn(cname, lit(default if default is not None else \"\"))\n",
    "        elif dtype == \"int\":\n",
    "            df = df.withColumn(cname, lit(default if default is not None else 0))\n",
    "        elif dtype == \"float\":\n",
    "            df = df.withColumn(cname, lit(default if default is not None else 0.0))\n",
    "        elif dtype == \"boolean\":\n",
    "            df = df.withColumn(cname, lit(default if default is not None else False))\n",
    "        elif dtype == \"timestamp\":\n",
    "            df = df.withColumn(cname,lit(default) if default is not None else current_timestamp())\n",
    "        else:\n",
    "            print(f\"Unsupported data type: {dtype}\")\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# def f_cast_col(df,schema_map):\n",
    "#     for cname,dtype in schema_map.items():\n",
    "#         df = df.withColumn(cname, col(cname).cast(dtype))\n",
    "#     return df\n",
    "def f_cast_col(df, schema_map):\n",
    "    for cname, dtype in schema_map.items():\n",
    "        if cname in df.columns:\n",
    "            df = df.withColumn(cname, col(cname).cast(dtype))\n",
    "        else:\n",
    "            print(f\"⚠️ Column not found: {cname}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "def deduplicate_with_window(df, partition_cols, order_col=None, order_type=\"asc\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Removes duplicates using window function.\n",
    "\n",
    "    Parameters:\n",
    "    df              : Input DataFrame\n",
    "    partition_cols  : List of columns to identify duplicates\n",
    "    order_col       : Column to decide which row to keep\n",
    "    order_type      : 'asc' or 'desc' (default: asc)\n",
    "\n",
    "    Returns:\n",
    "    Deduplicated DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Build window specification\n",
    "    if order_col:\n",
    "        if order_type == \"desc\":\n",
    "            window_spec = Window.partitionBy(*partition_cols).orderBy(col(order_col).desc())\n",
    "        else:\n",
    "            window_spec = Window.partitionBy(*partition_cols).orderBy(col(order_col))\n",
    "    else:\n",
    "        window_spec = Window.partitionBy(*partition_cols).orderBy(lit(1))\n",
    "\n",
    "    # Apply row_number\n",
    "    df_rn = df.withColumn(\"rn\", row_number().over(window_spec))\n",
    "\n",
    "    # Keep only first record\n",
    "    df_dedup = df_rn.filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "    return df_dedup\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "def deduplicate(df, keys):#pass the keys as a list []\n",
    "    return df.dropDuplicates(keys)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "def drop_required_nulls(df, columns):#pass the columns as a list []\n",
    "    return df.dropna(subset=columns)\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "def sum_two_columns(df, col1, col2, output_col):\n",
    "    return df.withColumn(output_col, col(col1) + col(col2))\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "def standardize_string_columns(df, rules):\n",
    "    \"\"\"\n",
    "    rules = {\n",
    "        \"column_name\": \"operation\",\n",
    "        \"column_name2\": \"operation\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    for column_name, operation in rules.items():\n",
    "\n",
    "        if operation == \"lower_trim\":\n",
    "            df = df.withColumn(column_name, lower(trim(col(column_name))))\n",
    "\n",
    "        elif operation == \"upper_trim\":\n",
    "            df = df.withColumn(column_name, upper(trim(col(column_name))))\n",
    "\n",
    "        elif operation == \"initcap_trim\":\n",
    "            df = df.withColumn(column_name, initcap(trim(col(column_name))))\n",
    "\n",
    "        elif operation == \"trim\":\n",
    "            df = df.withColumn(column_name, trim(col(column_name)))\n",
    "\n",
    "        elif operation == \"remove_spaces\":\n",
    "            df = df.withColumn(\n",
    "                column_name,\n",
    "                regexp_replace(col(column_name), \" \", \"\")\n",
    "            )\n",
    "\n",
    "        elif operation == \"null_if_empty\":\n",
    "            df = df.withColumn(\n",
    "                column_name,\n",
    "                when(trim(col(column_name)) == \"\", None)\n",
    "                .otherwise(col(column_name))\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported operation: {operation}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447e59f3-0ce6-4f23-a1ae-8992e5d5a13c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_liquid(df, table_name, cluster_cols):\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .clusterBy(*cluster_cols)\n",
    "          .saveAsTable(table_name)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "com_utl_fun",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a043d69a-6e35-4d80-9b5a-d7cc957679ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Return Spark session\n",
    "from pyspark.sql.session import SparkSession\n",
    "def get_spark_session(app_name=\"Some Anonymous Data Engineering Project\"):\n",
    "    try:\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark:\n",
    "            return spark\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (SparkSession.builder.appName(app_name).getOrCreate())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ed31e4-82c7-4cfb-b0d4-0ca19cd4f460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_file(spark,filetype,path,header=True,infer_schema=True,mline=True):\n",
    "    if filetype==\"csv\":\n",
    "        return spark.read.csv(path,header=header,inferSchema=infer_schema)#read_csv_df(spark,path)\n",
    "    elif filetype==\"json\":\n",
    "        return read_json_df(spark,path)\n",
    "    elif filetype==\"delta\":\n",
    "        return read_delta_df(spark,path)\n",
    "    elif filetype=='orc':\n",
    "        return spark.read.orc(path)\n",
    "    elif filetype=='parquet':\n",
    "        return spark.read.parquet(path)\n",
    "    else:\n",
    "        raise Exception(\"File type not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9111c9a1-2d8b-468a-8724-cf4cf34099ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_file(df, path,format=\"delta\", mode=\"overwrite\"):\n",
    "    return df.write.mode(mode).format(format).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21501e22-e056-4e05-8e96-2dec1decbed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def enrich_df(df):\n",
    "    return(df.withColumn(\"ingestion_time\",current_timestamp()))\n",
    "\n",
    "def f_cast_col(df,schema_map):\n",
    "    for cname,dtype in schema_map.items():\n",
    "        df = df.withColumn(cname, col(cname).cast(dtype))\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "def deduplicate_with_window(df, partition_cols, order_col=None, order_type=\"asc\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Removes duplicates using window function.\n",
    "\n",
    "    Parameters:\n",
    "    df              : Input DataFrame\n",
    "    partition_cols  : List of columns to identify duplicates\n",
    "    order_col       : Column to decide which row to keep\n",
    "    order_type      : 'asc' or 'desc' (default: asc)\n",
    "\n",
    "    Returns:\n",
    "    Deduplicated DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Build window specification\n",
    "    if order_col:\n",
    "        if order_type == \"desc\":\n",
    "            window_spec = Window.partitionBy(*partition_cols).orderBy(col(order_col).desc())\n",
    "        else:\n",
    "            window_spec = Window.partitionBy(*partition_cols).orderBy(col(order_col))\n",
    "    else:\n",
    "        window_spec = Window.partitionBy(*partition_cols).orderBy(lit(1))\n",
    "\n",
    "    # Apply row_number\n",
    "    df_rn = df.withColumn(\"rn\", row_number().over(window_spec))\n",
    "\n",
    "    # Keep only first record\n",
    "    df_dedup = df_rn.filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "    return df_dedup\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "def deduplicate(df, keys):#pass the keys as a list []\n",
    "    return df.dropDuplicates(keys)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "def drop_required_nulls(df, columns):#pass the columns as a list []\n",
    "    return df.dropna(subset=columns)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "com_utl_fun",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

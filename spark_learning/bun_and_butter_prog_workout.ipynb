{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6660416a-ee49-4d8e-b56a-3ed9af7a7fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data munging concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f118933d-ed70-4ba9-8767-75917add5780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession#15lakhs\n",
    "spark=SparkSession.builder.appName(\"WD37 - ETL Pipeline - Bread & Butter\").getOrCreate()#3 lakhs LOC by Databricks (for eg. display, delta, xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d3f96f2-f04f-4060-83fd-450482fb08c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.options(header=False, inferSchema=True).format(\"csv\").load(\"/Volumes/workspace/default/logan_datalake/customers_raw_more_rem_head.txt\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"location\",\"proffesion\")\n",
    "#display(df_raw.limit(10))\n",
    "display(df_raw.sample(.1))\n",
    "#df_raw.count()\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4ac746-b1f8-4e77-b135-98d3ccaf82bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import col,trim\n",
    "#display(df_raw.filter(df_raw.fname.isNull() | df_raw.lname.isNull())) # dsl\n",
    "#df_fnull = df_raw.where(df_raw.fname.isNull() | df_raw.lname.isNull()) # sel\n",
    "df_rm_null_names = df_raw.where(df_raw.fname.isNotNull() & df_raw.lname.isNotNull())\n",
    "#display(df_fnull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b24226db-9e91-4ce1-8ea0-40c28c2db67a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"actual count of the data\",df_raw.count())\n",
    "print(\"distinct on orginal cnt\",df_raw.distinct().count())\n",
    "print(\"distinct on id\",df_raw.select('id').distinct().count())\n",
    "print(\"fname,lname null removed\",df_rm_null_names.count())\n",
    "print(\"Dropping duplicates by id\",df_raw.dropDuplicates(['id']).count())\n",
    "print(\"Dropping duplicates in df\",df_raw.dropDuplicates().count())\n",
    "display(df_raw.describe())\n",
    "display(df_raw.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a9676f-81de-4c7d-a787-19f787e8928f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Extraction (Ingestion) methodologies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b2eb99-4687-4bed-8ab5-68fdf6c1709d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "header = \"id string,fname string,lname string,age string,location string,proffesion string\"\n",
    "#single file different header calling method\n",
    "##------------------------------------Calling multiple files-----------------------\n",
    "#raw_df = spark.read.format(\"csv\").load(\"/Volumes/workspace/default/logan_datalake/customers_raw_more_rem_head.txt\").toDF(*header.split(\",\"))\n",
    "##---------\n",
    "#raw_df = spark.read.schema(header).format(\"csv\").load(\"/Volumes/workspace/default/logan_datalake/customers_raw_more_rem_head.txt\",inferSchema=True,header=False)\n",
    "#------------------------------------Calling multiple files-----------------------\n",
    "raw_df = spark.read.schema(header).csv(path=\"/Volumes/workspace/default/logan_datalake/customers_raw_more_rem_head.txt\",inferSchema=True,header=False)\n",
    "raw_df1 = spark.read.schema(header).csv(path=\"/Volumes/workspace/default/logan_datalake/customers_raw_more.txt\",inferSchema=True,header=False)\n",
    "raw_df2 = spark.read.schema(header).csv(path=\"/Volumes/workspace/default/logan_datalake/customers_raw_miss_col.txt\",inferSchema=True,header=False)\n",
    "#,recursiveFileLookup=True,pathGlobFilter=\"custsm*\") this is used for multiple files in different folders\n",
    "#raw_df = spark.read.schema(header).csv(path=[\"/Volumes/workspace/default/logan_datalake/customers_raw_more_rem_head.txt\",\"/Volumes/workspace/default/logan_datalake/customers_raw_more.txt\"],inferSchema=True,header=False)\n",
    "\n",
    "print('raw data cnt',raw_df.count(),raw_df1.count(),raw_df2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeccc220-735f-462f-8824-7de216d2c689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Active Data munging..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae9e55f-5b7f-4b5e-84e3-d861484ce068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#union/merged\n",
    "#df_union = raw_df.union(raw_df1)\n",
    "df_union1 = raw_df.unionByName(raw_df2,allowMissingColumns=True)#unionByName is used for different columns\n",
    "print('union data cnt',df_union1.count())\n",
    "#display(df_union1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9667db2c-9bf8-4bd1-a5d9-7b10d9fbaa13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,NumericType\n",
    "#Validation by doing cleansing line 178\n",
    "#mode=\"permissive\" (DEFAULT)\n",
    "        #Allow bad rows, do not fail, do not drop\n",
    "        #Put invalid/broken row content into a special column\n",
    "#mode='dropMalformed'\n",
    "        #no corrupt column, no nulls — Spark removes the entire row\n",
    "        #If a row doesn't match schema → it is thrown away\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema_df = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"fname\", StringType(), True),\n",
    "    StructField(\"lname\", StringType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"proffesion\", StringType(), True)\n",
    "])\n",
    "#mode=\"permissive\"\n",
    "df_modes_permissive = spark.read.schema(schema_df).csv(\n",
    "    path=\"/Volumes/workspace/default/logan_datalake/customers_raw_more_rem_head.txt\",\n",
    "    header=False,columnNameOfCorruptRecord=\"corrupt_record\"\n",
    ")\n",
    "\n",
    "df_modes_dropMalformed = spark.read.schema(schema_df).csv(\n",
    "    path=\"/Volumes/workspace/default/logan_datalake/customers_raw_more_rem_head.txt\",\n",
    "    header=False,mode=\"dropMalformed\"\n",
    ")\n",
    "\n",
    "print(\"Permissive mode cnt\",df_modes_permissive.count())\n",
    "print(\"DropMalformed mode cnt\",df_modes_dropMalformed.count())\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\",len(df_modes_dropMalformed.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0afe0f5-fad9-4ddd-a5b6-7f7f79a4a1b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " \n",
    "#####Rejection Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31449167-9014-40ac-b335-90992c918891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema_df = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"fname\", StringType(), True),\n",
    "    StructField(\"lname\", StringType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"proffesion\", StringType(), True),\n",
    "    StructField(\"corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_modes_permissive = spark.read.schema(schema_df).csv(\n",
    "    path=\"/Volumes/workspace/default/logan_datalake/customers_raw_more_rem_head.txt\",\n",
    "    header=False,columnNameOfCorruptRecord=\"corrupt_record\"\n",
    ")\n",
    "#display(df_modes_permissive)\n",
    "#display(df_modes_permissive.where(df_modes_permissive.corrupt_record.isNull()))#cleaned rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "529422a1-bf6e-459c-8c31-658486186683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "#Important na functions we can use to do cleansing.\n",
    "schema_df = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"fname\", StringType(), True),\n",
    "    StructField(\"lname\", StringType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"proffesion\", StringType(), True)])\n",
    "\n",
    "df_raw = spark.read.schema(schema_df).csv(\n",
    "    path=\"/Volumes/workspace/default/logan_datalake/customers_raw_more_rem_head.txt\",\n",
    "    header=False)\n",
    "#This function will drop any column in a given row with null otherwise this function returns rows with no null columns - In a scenario of if the source send the Datascience Model features (we shouldn't have any one feature with null value, hence we can use this function)\n",
    "cleanseddf=df_raw.na.drop(how=\"any\")\n",
    "cleanseddf1=df_raw.na.drop(how=\"any\",subset=[\"id\",\"age\"])\n",
    "#display(df_raw)\n",
    "#display(cleanseddf1)\n",
    "#display(cleanseddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc5c9124-e24d-4ff8-af7a-4addf04ae9fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Scrubbing \n",
    "na.fill() & na.replace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814a4c33-0d64-410d-96ce-4ff138b4ea4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#df_fill = df_raw.na.fill({'id': 0, 'age': 0,'fname': 'missing','lname': 'missing','proffesion': 'Not'})  \n",
    "df_fill=df_raw.na.fill('not provided',subset=[\"lastname\",\"profession\"])# this will replace null with given val\n",
    "#df_replace = df_raw.na.replace(['chennai'], ['che'], 'location') \n",
    "find_replace_values_dict1={'chennai':'Chennai','delhi':'Head off'}\n",
    "df_replace = df_raw.na.replace(find_replace_values_dict1,subset=['location'])\n",
    " \n",
    "#display(df_fill)\n",
    "#display(df_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ad7e89e-d37d-42ed-9691-8cbdd92e3836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####DeDuplication\n",
    "Removal of duplicate rows/columns based on a priority or non priority\n",
    "distinct & dropDuplicates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e3cd56-498e-4d38-a551-4e79faed72b2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767160506123}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_raw.select(\"id\").groupBy(\"id\").count().filter(\"count > 1\"))\n",
    "\n",
    "display(df_raw.select(\"*\").where(\"id = 1010 or id is null\"))\n",
    "dedupdf2=df_raw.dropDuplicates(subset=[\"id\"])\n",
    "\n",
    "display(dedupdf2.select(\"*\").where(\"id = 1010 or id is null\"))\n",
    "\n",
    "display(df_raw.dropDuplicates(subset=[\"id\"]).select(\"*\").where(\"id = 1010 or id is null\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bun_and_butter_prog_workout",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
